{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n",
    "def hash_row(row):\n",
    "    row_string = str(row['Application_Number'])\n",
    "    return hashlib.sha256(row_string.encode()).hexdigest()\n",
    "\n",
    "df_raw['Application_Number_hashed'] = df_raw.apply(hash_row, axis=1)\n",
    "df_raw.to_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131_withhasedappId.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_model_path = '/d/shared/silver_projects_v2/origencepoc/autoloanv4/modeling/model_artifacts/autoloan/model1Ensemble'\n",
    "scores_df = pd.read_parquet(os.path.join(final_model_path, 'penfed_autoindirectv1_scores.parquet'))\n",
    "ana_vs40 = pd.read_parquet('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/processed/supporting_artifacts/raw_analysis_data_with _VS40.parquet')\n",
    "ana_raw = pd.read_parquet('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/processed/supporting_artifacts/analysis_data.parquet')\n",
    "ana = ana_raw.merge(ana_vs40[['ZEST_KEY', 'EFX_VS40']].set_index('ZEST_KEY'), how='left', left_index=True, right_index=True)\n",
    "\n",
    "ana = ana.merge(scores_df[['final_model_predictions']], how='left', left_index=True, right_index=True)\n",
    "# ana.to_parquet('./final_merged_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book = ana[ana['flgFunded']==1]\n",
    "df_unbook = ana[ana['flgFunded']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mob_24_120\n",
       "0.0    102005\n",
       "1.0      6672\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_loans_df['mob_24_120'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_loans_df = pd.read_parquet('./auto_proxy_targets.parquet')\n",
    "\n",
    "df_unbook_proxy = df_unbook.drop(columns=['appId', 'flgFunded', 'flgApproved','Funded_Date','appDate','mob_3_30',\n",
    " 'mob_4_60',\n",
    " 'mob_5_60',\n",
    " 'mob_6_60',\n",
    " 'mob_6_90',\n",
    " 'mob_12_90',\n",
    " 'mob_12_120',\n",
    " 'mob_18_90',\n",
    " 'mob_18_120',\n",
    " 'mob_24_90',\n",
    " 'mob_24_120']).merge(proxy_loans_df.set_index('ZEST_KEY'), left_index = True, \n",
    "                                  right_index = True,\n",
    "                                  how = 'left').reset_index(drop = False)\n",
    "df_unbook_proxy['abs_appdate_minus_opendate'] = (df_unbook_proxy['Funded_Date'] - df_unbook_proxy['appDate']).abs().dt.days\n",
    "df_unbook_w_targets = df_unbook_proxy.sort_values(['ZEST_KEY', 'abs_appdate_minus_opendate', 'mob_6_60'],\n",
    "                                   ascending = [True, True, False]).drop_duplicates(['ZEST_KEY'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_all_merge = pd.read_csv('/d/shared/users/yyz/penfed/proxy_target_part1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((497471, 181), (166029, 219), (518565, 384), (497471, 384))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unbook.shape, proxy_loans_df.shape, df_unbook_proxy.shape,df_unbook_w_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unbook_w_targets['DATE_REPORTED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220/1885037264.py:1: DtypeWarning: Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(631588, 126)\n",
      "(606073, 126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decision\n",
       "Withdraw    258114\n",
       "Decline     197700\n",
       "Funded      125847\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = df_raw.copy()\n",
    "df = df[df['Application_Status'].isin(['Refer', 'Processing', 'Exception'])==False]\n",
    "print(df.shape) # 561,137\n",
    "df = df[df['Max_Credit_Risk_Score']>=610] # 537656\n",
    "print(df.shape) # 537, 652\n",
    "\n",
    "df = df[~((df['PenFed_Funded']==1) & (df['Lendpro_Cert_no'].notnull()))] # exclude insured\n",
    "df = df[~((df['PenFed_Funded']==1) & (df['cancelled_flag']==1))] # exclude cancelled\n",
    "df = df[~((df['PenFed_Funded']==1) & (df['never_cash_flag']==1))] # exclude never cash the check\n",
    "df = df[~((df['PenFed_Funded']==1) & (df['fraud_3rd_party']==1))] # exclude 3rd party fraud\n",
    "\n",
    "\n",
    "df['Decision'] = None\n",
    "df['Decision'] = np.where(df['PenFed_Funded']==1, 'Funded', df['Decision'])\n",
    "df['Decision'] = np.where((df['PenFed_Funded']==0) & \n",
    "                          ((df['Application_Status']=='Approve')|((df['Application_Status']=='Counter')&(df['Adverse_Action_Reasons'].isnull()))), 'Withdraw', df['Decision'])\n",
    "df['Decision'] = np.where((df['PenFed_Funded']==0) & \n",
    "                          ((df['Application_Status']=='Decline')|((df['Application_Status']=='Counter')&(df['Adverse_Action_Reasons'].notnull()))), 'Decline', df['Decision'])\n",
    "\n",
    "\n",
    "df['Decision'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n",
    "df_raw['Decision'] = None\n",
    "df_raw['Decision'] = np.where(df_raw['PenFed_Funded']==1, 'Funded', df_raw['Decision'])\n",
    "df_raw['Decision'] = np.where((df_raw['PenFed_Funded']==0) & \n",
    "                          ((df_raw['Application_Status']=='Approve')|((df_raw['Application_Status']=='Counter')&(df_raw['Adverse_Action_Reasons'].isnull()))), 'Withdraw', df_raw['Decision'])\n",
    "df_raw['Decision'] = np.where((df_raw['PenFed_Funded']==0) & \n",
    "                          ((df_raw['Application_Status']=='Decline')|((df_raw['Application_Status']=='Counter')&(df_raw['Adverse_Action_Reasons'].notnull()))), 'Decline', df_raw['Decision'])\n",
    "\n",
    "df_raw['flgBorrower'] = df_raw['Role_Code'].map({'PRIMARY': 1, 'JOINTOWN': 2})\n",
    "df_raw['ZEST_KEY'] = df_raw['Application_Number'].astype(str)+'_'+df_raw['flgBorrower'].astype(str)+'_229_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154338, 181), (497471, 384))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_book.shape, df_unbook_w_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = [\n",
    "               'ZEST_KEY', 'Funded_Date','mob_6_60', 'mob_12_90', 'mob_12_120','mob_18_90','mob_18_120','mob_24_120',\n",
    "               'final_model_predictions', 'EFX_VS40'\n",
    "              ]\n",
    "\n",
    "df_all = pd.concat([df_book.reset_index(drop=False)[common_cols], df_unbook_w_targets[common_cols]]).reset_index(drop=False)\n",
    "\n",
    "drop_common_cols = list(set(df_all.columns)&set(df_raw.columns)-set(['ZEST_KEY', 'Decision']))\n",
    "\n",
    "df_merged = df_raw.drop(columns=drop_common_cols).merge(df_all, how='left', on=['ZEST_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "penfed_new = pd.read_csv('./data_3rd_party_refi_addvar_sent_20250613210112.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merged = penfed_new.merge(df_merged, how='inner', on=['Application_Number', 'Application_Date', 'Role_Code']).drop(columns=['index'])\n",
    "full_merged = full_merged.drop_duplicates()\n",
    "full_merged.to_csv('/d/shared/users/yyz/penfed/data_3rd_party_refi_f_sent_withunfundedtarget_zestscore_vs4_20250618.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Application_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458991</th>\n",
       "      <td>88800617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459697</th>\n",
       "      <td>88766767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500542</th>\n",
       "      <td>89236327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Application_Number\n",
       "458991            88800617\n",
       "459697            88766767\n",
       "500542            89236327"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw[df_raw.duplicated(['Application_Number','Application_Date','Role_Code'])][['Application_Number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651816, 142)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('/d/shared/users/yyz/penfed/data_3rd_party_refi_f_sent_withunfundedtarget_zestscore_vs4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appmonth\n",
       "2021-01      79.0\n",
       "2021-02     108.0\n",
       "2021-03     149.0\n",
       "2021-04     189.0\n",
       "2021-05     213.0\n",
       "2021-06     346.0\n",
       "2021-07     400.0\n",
       "2021-08     460.0\n",
       "2021-09     518.0\n",
       "2021-10     646.0\n",
       "2021-11     859.0\n",
       "2021-12     928.0\n",
       "2022-01     982.0\n",
       "2022-02     935.0\n",
       "2022-03    1341.0\n",
       "2022-04    1241.0\n",
       "2022-05    1268.0\n",
       "2022-06    1245.0\n",
       "2022-07     555.0\n",
       "2022-08     303.0\n",
       "2022-09     192.0\n",
       "2022-10     237.0\n",
       "2022-11     219.0\n",
       "2022-12     220.0\n",
       "2023-01     235.0\n",
       "2023-02     245.0\n",
       "2023-03     250.0\n",
       "2023-04      39.0\n",
       "2023-05      10.0\n",
       "2023-06       1.0\n",
       "2023-07       0.0\n",
       "2023-08       0.0\n",
       "2023-09       0.0\n",
       "2023-10       0.0\n",
       "2023-11       0.0\n",
       "2023-12       0.0\n",
       "2024-01       0.0\n",
       "2024-02       0.0\n",
       "2024-03       0.0\n",
       "2024-04       0.0\n",
       "2024-05       0.0\n",
       "2024-06       0.0\n",
       "2024-07       0.0\n",
       "2024-08       0.0\n",
       "2024-09       0.0\n",
       "2024-10       0.0\n",
       "2024-11       0.0\n",
       "2024-12       0.0\n",
       "2025-01       0.0\n",
       "2025-02       0.0\n",
       "2025-03       0.0\n",
       "2025-04       0.0\n",
       "Name: mob_24_120, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_merged['mob_24_120'] = full_merged['mob_24_120'].astype(float)\n",
    "full_merged['appmonth'] = full_merged['Application_Date'].str[:7]\n",
    "full_merged.groupby('appmonth')['mob_24_120'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.2.1', 'v1.12.4')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ztarget\n",
    "import model_engine\n",
    "import json\n",
    "\n",
    "ztarget.__version__, model_engine.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651809, 179)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zest analysis data \n",
    "ana = pd.read_parquet('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/processed/supporting_artifacts/analysis_data.parquet')\n",
    "ana.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163/4184390068.py:2: DtypeWarning: Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(651816, 126)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PenFed Sent Data\n",
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131.csv')\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfunded Proxy Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ztarget import EQTargetGenerator, TUTargetGenerator, EXPTargetGenerator\n",
    "from ztarget.target_generator import final_target\n",
    "from model_engine.assets.utils import load_asset\n",
    "from itertools import product\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3://power-client-data-staging/CLIENT/PARSED/DATA/BUREAU%3Dequifax/FORMAT%3Dcms_6/TABLE%3Dtrade/PULL_NAME%3D20250529_penfed/ARCHIVE_DATE%3D2025-04-30/'.replace('%3D', '=')\n",
    "perf_trade = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfunded_ana = ana[ana.flgFunded == 0]\n",
    "unfunded_ana.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfunded_trade = perf_trade[perf_trade.ZEST_KEY.isin(unfunded_ana.index)]\n",
    "unfunded_trade.ZEST_KEY.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Part 1 – Auto Loan Proxy\n",
    "\n",
    "Return a record for all Auto Loans (exclude leases) opened within -1/+3 months from app month\n",
    "\n",
    "Fields returned for each proxy trade:\n",
    "- MOB6 Ever 60+DPD/BNK/Repossession/Charged-off\n",
    "- MOB12 Ever 90+DPD/BNK/Repossession/Charged-off (General Bad PenFed used for model development)\n",
    "- MOB12 Ever 120+DPD/Repossession/Charged-off (Simulate PenFed current Charge off Policy. Does not include BNK)\n",
    "- MOB18 Ever 90+DPD/BNK/Repossession/Charged-off\n",
    "- MOB18 Ever 120+DPD/Repossession/Charged-off\n",
    "- MOB24 Ever 120+DPD/Repossession/Charged-off\n",
    "- Payment history grid (pulled from current archive)\n",
    "\t\n",
    "If there are multiple proxy trades for an individual application, we want to keep all of them as reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_trade_json = load_asset('equifax/cms_6/trade.json')\n",
    "eq_trade_json_nobnk = load_asset('equifax/cms_6/trade.json') # remove 7 (Included in Chapter 13) from Charge-off\n",
    "eq_trade_json_nobnk['info']['payment_patterns'] = {'patterns': ['PAYMENT_HISTORY_1_24',\n",
    "  'PAYMENT_HISTORY_25_36',\n",
    "  'PAYMENT_HISTORY_37_48'],\n",
    " 'rate': {'paid_as_agreed': ['0', '1'],\n",
    "  'DQ30': ['2'],\n",
    "  'DQ60': ['3'],\n",
    "  'DQ90': ['4'],\n",
    "  'DQ120': ['5'],\n",
    "  'CO': ['5', '6', '8', '9', 'G', 'H', 'K', 'L']},\n",
    " 'trim': 48,\n",
    " 'placeholder': '/',\n",
    " 'keep': ['DQ30', 'DQ60', 'DQ90', 'DQ120', 'CO']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqs = [\"DQ30\", \"DQ60\", \"DQ90\", \"DQ120\", \"CO\"]\n",
    "time_window = [3, 6, 9, 12, 15, 18, 24, 30, 36]\n",
    "default_proxy_time_window = [-30, 90]\n",
    "\n",
    "# exclude 3A: Auto Lease \n",
    "# exclude 11: recreational merchandise\n",
    "\n",
    "proxy_account_type = ['00']\n",
    "target_generator = EQTargetGenerator(join_key='ZEST_KEY', app_id='appId', app_date='appDate', \n",
    "                                      proxy_time_window = default_proxy_time_window, \n",
    "                                      proxy_account_type = proxy_account_type,\n",
    "                                      **eq_trade_json['info'],\n",
    "                                     proxy = True)\n",
    "target_generator.proxy_account_type\n",
    "\n",
    "target_generator_nobnk = EQTargetGenerator(join_key='ZEST_KEY', app_id='appId', app_date='appDate', \n",
    "                                      proxy_time_window = default_proxy_time_window, \n",
    "                                      proxy_account_type = proxy_account_type,\n",
    "                                      **eq_trade_json_nobnk['info'],\n",
    "                                     proxy = True)\n",
    "target_generator.proxy_account_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['appId', 'appDate', 'flgFunded', 'flgApproved',]\n",
    "unfunded_trade = unfunded_trade.merge(unfunded_ana[cols], left_on = 'ZEST_KEY', \n",
    "                       right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transforming date formats ---\n",
      "Warning! Cannot covert CLOSED_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_BEFORE_HISTORY to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert LAST_PAYMENT_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DFD_DLA to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_3 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DMD_REPORTED to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DEFERRED_PAYMENT_START_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_2 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_1 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "--- Selecting loan types in ['00'] ---\n",
      "--- Selecting loan periods within [-30, 90] days ---\n",
      "--- Implementing additional data preprocess ---\n",
      "--- Getting DQ/CO dates from: ---\n",
      "    get_coAmt_dq_date\n",
      "    get_code_dq_date\n",
      "    get_dmd_dq_date\n",
      "    get_highRate_dq_date\n",
      "    get_pattern_dq_date\n",
      "--- Implementing additional proxy date postprocess ---\n"
     ]
    }
   ],
   "source": [
    "proxy_date_df = target_generator.get_dqDates(unfunded_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transforming date formats ---\n",
      "Warning! Cannot covert CLOSED_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_BEFORE_HISTORY to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert LAST_PAYMENT_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DFD_DLA to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_3 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DMD_REPORTED to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DEFERRED_PAYMENT_START_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_2 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_1 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "--- Selecting loan types in ['00'] ---\n",
      "--- Selecting loan periods within [-30, 90] days ---\n",
      "--- Implementing additional data preprocess ---\n",
      "--- Getting DQ/CO dates from: ---\n",
      "    get_coAmt_dq_date\n",
      "    get_code_dq_date\n",
      "    get_dmd_dq_date\n",
      "    get_highRate_dq_date\n",
      "    get_pattern_dq_date\n",
      "--- Implementing additional proxy date postprocess ---\n"
     ]
    }
   ],
   "source": [
    "proxy_date_df_nobnk = target_generator_nobnk.get_dqDates(unfunded_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy_date_df.to_parquet('/d/shared/users/lyt/penfed_poc1_unfunded_proxy_targets/full_auto_loan_proxy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_date_df_nobnk.to_parquet('/d/shared/users/yyz/penfed/full_auto_loan_proxy_nobnk.parquet')\n",
    "proxy_date_df.to_parquet('/d/shared/users/yyz/penfed/full_auto_loan_proxy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166029, 106)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_date_df = pd.read_parquet('/d/shared/users/yyz/penfed/full_auto_loan_proxy.parquet')\n",
    "proxy_date_df_nobnk = pd.read_parquet('/d/shared/users/yyz/penfed/full_auto_loan_proxy_nobnk.parquet')\n",
    "\n",
    "proxy_date_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf': {'BUREAU': 'equifax',\n",
       "  'FORMAT': 'cms_6',\n",
       "  'PULL_DATE': '2025-05-29',\n",
       "  'PULL_NAME': '20250529_penfed',\n",
       "  'PERFORMANCE_DATE': '2025-04-30'}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/bureau_config.json\", \"r\") as file: \n",
    "    config = json.load(file)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "pull_date = config['conf']['PULL_DATE']\n",
    "proxy_target = target_generator.generate_target(proxy_date_df, \n",
    "                                         target=dqs, \n",
    "                                         time_window=time_window, \n",
    "                                         query_date= pull_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "proxy_target_nobnk = target_generator_nobnk.generate_target(proxy_date_df_nobnk, \n",
    "                                         target=dqs, \n",
    "                                         time_window=time_window, \n",
    "                                         query_date=pull_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166029, 408), (166029, 408))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target.shape, proxy_target_nobnk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "bnk_column_mapping = \\\n",
    "    {'proxy_DQ60_m6': 'mob_6_60',\n",
    "     'proxy_DQ60_m12': 'mob_12_60',\n",
    "      'proxy_DQ90_m12': 'mob_12_90',\n",
    "      'proxy_DQ90_m18': 'mob_18_90',\n",
    "     'proxy_DQ120_m12': 'mob_12_120',\n",
    "    'proxy_DQ120_m18': 'mob_18_120',\n",
    "    'proxy_DQ120_m24': 'mob_24_120',\n",
    "      'proxy_DQ60_m6_status': 'mob_6_60_status',\n",
    "      'proxy_DQ60_m12_status': 'mob_12_60_status',\n",
    "      'proxy_DQ90_m12_status': 'mob_12_90_status',\n",
    "      'proxy_DQ90_m18_status': 'mob_18_90_status',\n",
    "     'proxy_DQ120_m12_status': 'mob_12_120_status',\n",
    "    'proxy_DQ120_m18_status': 'mob_18_120_status',\n",
    "    'proxy_DQ120_m24_status': 'mob_24_120_status',\n",
    "}\n",
    "\n",
    "nobnk_column_mapping = {\n",
    "    'proxy_DQ120_m12': 'mob_12_120_nobnk',\n",
    "    'proxy_DQ120_m18': 'mob_18_120_nobnk',\n",
    "    'proxy_DQ120_m24': 'mob_24_120_nobnk',\n",
    "    'proxy_DQ120_m12_status': 'mob_12_120_nobnk_status',\n",
    "    'proxy_DQ120_m18_status': 'mob_18_120_nobnk_status',\n",
    "    'proxy_DQ120_m24_status': 'mob_24_120_nobnk_status',\n",
    "}\n",
    "\n",
    "date_source_cols = [c for c in proxy_target.columns if c.endswith('date_source')]\n",
    "\n",
    "proxy_target_rename = proxy_target.rename(columns=bnk_column_mapping)\n",
    "proxy_target_nobnk_rename = proxy_target_nobnk.rename(columns=nobnk_column_mapping)\n",
    "\n",
    "proxy_target_nobnk_keep_cols = list(nobnk_column_mapping.values())\n",
    "\n",
    "proxy_target_all = pd.concat([proxy_target_rename, \\\n",
    "                                       proxy_target_nobnk_rename[proxy_target_nobnk_keep_cols]\n",
    "                                       ], axis=1)\n",
    "\n",
    "keep_cols = ['ZEST_KEY','DATE_OPENED','DATE_REPORTED','ACCOUNT_TYPE','HIGH_CREDIT','SCHEDULED_PAYMENT_AMOUNT','TERMS_DURATION','TERMS_FREQUENCY',\\\n",
    "             'PAYMENT_HISTORY_1_24','PAYMENT_HISTORY_25_36','PAYMENT_HISTORY_37_48'] + \\\n",
    "            list(bnk_column_mapping.values())+list(nobnk_column_mapping.values())\n",
    "proxy_target_all_subset = proxy_target_all[keep_cols]\n",
    "\n",
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131_withhasedappId.csv')\n",
    "\n",
    "df_raw['flgBorrower'] = df_raw['Role_Code'].map({'PRIMARY': 1, 'JOINTOWN': 2})\n",
    "df_raw['ZEST_KEY'] = df_raw['Application_Number'].astype(str)+'_'+df_raw['flgBorrower'].astype(str)+'_229_22'\n",
    "keep_cols = ['ZEST_KEY','Application_Number','Application_Number_hashed','Application_Date','Role_Code','Seller_Name','Product','Application_Status','PenFed_Funded']\n",
    "df_raw = df_raw[keep_cols]\n",
    "proxy_target_all_merge = df_raw.merge(proxy_target_all_subset, how='inner', on=['ZEST_KEY'])\\\n",
    "    .drop(columns=['ZEST_KEY','mob_12_120','mob_18_120','mob_24_120','mob_12_120_status','mob_18_120_status','mob_24_120_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166029, 32), (166029, 31))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_all_merge.shape, proxy_target_all_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166029, 31), (166029, 31))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_all_merge.shape, proxy_target_all_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_all_merge.to_csv('/d/shared/users/yyz/penfed/proxy_target_part1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appmonth\n",
       "2021-01    35.0000\n",
       "2021-02    54.0000\n",
       "2021-03    74.0000\n",
       "2021-04   106.0000\n",
       "2021-05   118.0000\n",
       "2021-06   193.0000\n",
       "2021-07   174.0000\n",
       "2021-08   228.0000\n",
       "2021-09   227.0000\n",
       "2021-10   334.0000\n",
       "2021-11   377.0000\n",
       "2021-12   411.0000\n",
       "2022-01   431.0000\n",
       "2022-02   411.0000\n",
       "2022-03   585.0000\n",
       "2022-04   512.0000\n",
       "2022-05   573.0000\n",
       "2022-06   535.0000\n",
       "2022-07   327.0000\n",
       "2022-08   154.0000\n",
       "2022-09   110.0000\n",
       "2022-10   132.0000\n",
       "2022-11   109.0000\n",
       "2022-12   101.0000\n",
       "2023-01   109.0000\n",
       "2023-02   115.0000\n",
       "2023-03    80.0000\n",
       "2023-04    46.0000\n",
       "2023-05    10.0000\n",
       "2023-06     1.0000\n",
       "2023-07     0.0000\n",
       "2023-08     0.0000\n",
       "2023-09     0.0000\n",
       "2023-10     0.0000\n",
       "2023-11     0.0000\n",
       "2023-12     0.0000\n",
       "2024-01     0.0000\n",
       "2024-02     0.0000\n",
       "2024-03     0.0000\n",
       "2024-04     0.0000\n",
       "2024-05     0.0000\n",
       "2024-06     0.0000\n",
       "2024-07     0.0000\n",
       "2024-08     0.0000\n",
       "2024-09     0.0000\n",
       "2024-10     0.0000\n",
       "2024-11     0.0000\n",
       "2024-12     0.0000\n",
       "2025-01     0.0000\n",
       "2025-02     0.0000\n",
       "2025-03     0.0000\n",
       "Name: mob_24_120_nobnk, dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_all_merge['appmonth'] = proxy_target_all_merge['Application_Date'].str[:7]\n",
    "proxy_target_all_merge.groupby('appmonth')['mob_24_120_nobnk'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appmonth\n",
       "2021-01    35.0000\n",
       "2021-02    54.0000\n",
       "2021-03    74.0000\n",
       "2021-04   106.0000\n",
       "2021-05   118.0000\n",
       "2021-06   193.0000\n",
       "2021-07   174.0000\n",
       "2021-08   228.0000\n",
       "2021-09   227.0000\n",
       "2021-10   334.0000\n",
       "2021-11   377.0000\n",
       "2021-12   411.0000\n",
       "2022-01   431.0000\n",
       "2022-02   411.0000\n",
       "2022-03   585.0000\n",
       "2022-04   512.0000\n",
       "2022-05   573.0000\n",
       "2022-06   535.0000\n",
       "2022-07   327.0000\n",
       "2022-08   154.0000\n",
       "2022-09   110.0000\n",
       "2022-10   132.0000\n",
       "2022-11   109.0000\n",
       "2022-12   101.0000\n",
       "2023-01   109.0000\n",
       "2023-02   115.0000\n",
       "2023-03    80.0000\n",
       "2023-04    46.0000\n",
       "2023-05    10.0000\n",
       "2023-06     1.0000\n",
       "2023-07     0.0000\n",
       "2023-08     0.0000\n",
       "2023-09     0.0000\n",
       "2023-10     0.0000\n",
       "2023-11     0.0000\n",
       "2023-12     0.0000\n",
       "2024-01     0.0000\n",
       "2024-02     0.0000\n",
       "2024-03     0.0000\n",
       "2024-04     0.0000\n",
       "2024-05     0.0000\n",
       "2024-06     0.0000\n",
       "2024-07     0.0000\n",
       "2024-08     0.0000\n",
       "2024-09     0.0000\n",
       "2024-10     0.0000\n",
       "2024-11     0.0000\n",
       "2024-12     0.0000\n",
       "2025-01     0.0000\n",
       "2025-02     0.0000\n",
       "2025-03     0.0000\n",
       "Name: mob_24_120_nobnk, dtype: float64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_all_merge['appmonth'] = proxy_target_all_merge['Application_Date'].str[:7]\n",
    "proxy_target_all_merge.groupby('appmonth')['mob_24_120_nobnk'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Part 2 – Waterfall Proxy – Only Proceed to this step if no data is found in Part 1 \n",
    "\n",
    "Return one record max per applicant (the first that is found in the waterfall)\n",
    "\n",
    "1. Non-Mortgage Secured Loan opened -1 to 1 month from application month\n",
    "2. Non-Mortgage Secured Loan opened -1 to 3 month from application month\n",
    "3. Non-Mortgage Secured Loan opened -3 to 3 month from application month \n",
    "4. Existing Auto Loan (must be paid as agreed at app month)\n",
    "5. Existing Personal Loan (must be paid as agreed at app month)\n",
    "6. Any Open Loan (must be paid as agreed at app month)\n",
    "\n",
    "Duplicate treatment logic (for Part 2 only)\n",
    "Choose worst performing trade (60+DPD M12), next choose trade opened closest to app month \n",
    "Else random select from duplicate trades\n",
    "\n",
    "Fields returned for the single proxy, if found:\n",
    "Waterfall # that generated the trade \n",
    "- Date Opened, Loan Amount, Terms, Estimated APR\n",
    "- MOB6 Ever 60+DPD/BNK/Repossession/Charged-off\n",
    "- MOB12 Ever 90+DPD/BNK/Repossession/Charged-off (General Bad PenFed used for model development)\n",
    "- MOB12 Ever 120+DPD/Repossession/Charged-off (Simulate PenFed current Charge off Policy. Does not include BNK)\n",
    "- MOB18 Ever 90+DPD/BNK/Repossession/Charged-off\n",
    "- MOB18 Ever 120+DPD/Repossession/Charged-off\n",
    "- MOB24 Ever 120+DPD/Repossession/Charged-off\n",
    "- Payment history grid (pulled from current archive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def udpate_date_formats(data, dates): \n",
    "    for date_col, date_format in dates.items():\n",
    "        if np.issubdtype(data[date_col].dtype, np.datetime64):\n",
    "            print(f'{date_col} is already of datetime format! Will skip the converting process')\n",
    "            continue\n",
    "        data[date_col] = data[date_col].astype('str')\n",
    "        for date_col, date_format in dates.items(): \n",
    "            if date_format == '%m%d%Y':\n",
    "                idx = data[date_col].str[2:4] == '00'\n",
    "                data.loc[idx, date_col] = data.loc[idx, date_col].str[0:2] + '01' + data.loc[idx, date_col].str[4:]\n",
    "                data[date_col] = data[date_col].replace('        ', np.nan)\n",
    "            try:\n",
    "                data[date_col] = pd.to_datetime(data[date_col], format=date_format, errors='raise')\n",
    "            except:\n",
    "                print(f'Warning! Cannot covert {date_col} to datetime using {date_format} format! Please check the asset dates format!' )\n",
    "                print('Trying to convert datetime using inferred datetime format...')\n",
    "                data[date_col] = pd.to_datetime(data[date_col],format=date_format, errors='coerce')\n",
    "    return data         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_date_df = pd.read_parquet('/d/shared/users/yyz/penfed/full_auto_loan_proxy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Non-Mortgage Secured Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfunded_ana_v1 = unfunded_ana[~unfunded_ana.index.isin(proxy_date_df.ZEST_KEY)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02: secured\n",
    "# 22: Secured By Household Goods\n",
    "# 23: Secured By Household Goods/Collateral\n",
    "\n",
    "secured_account_type = ['02', '22', '23']\n",
    "\n",
    "cols = ['ZEST_KEY', 'appDate']\n",
    "unfunded_trade_v1 = unfunded_trade[unfunded_trade.ZEST_KEY.isin(unfunded_ana_v1.index)]\n",
    "unfunded_trade_v1 = unfunded_trade_v1.merge(unfunded_ana_v1.reset_index(drop=False)[cols], how='left', on=['ZEST_KEY'])\n",
    "dates = {\n",
    "    'DATE_REPORTED': '%m%d%Y',\n",
    "    'DATE_OPENED': '%m%d%Y',\n",
    "}\n",
    "unfunded_trade_v1 = udpate_date_formats(unfunded_trade_v1, dates)\n",
    "\n",
    "unfunded_trade_v1_filter = unfunded_trade_v1[(unfunded_trade_v1.ACCOUNT_TYPE.isin(secured_account_type))&(unfunded_trade_v1.PORTFOLIO_TYPE != 'M')]\n",
    "unfunded_trade_v1_filter['days_diff'] = (unfunded_trade_v1_filter['DATE_OPENED'] - unfunded_trade_v1_filter['appDate']).dt.days\n",
    "unfunded_trade_v1_filter = unfunded_trade_v1_filter[(unfunded_trade_v1_filter['days_diff']>=-90)&(unfunded_trade_v1_filter['days_diff']<=90)]\n",
    "\n",
    "\n",
    "def classify_loan_timing(days_diff):\n",
    "    if -30 <= days_diff <= 30:\n",
    "        return 1\n",
    "    elif -30 <= days_diff <= 90:\n",
    "        return 2\n",
    "    elif -90 <= days_diff <= 90:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan  # or 0 or any other indicator for not falling into these windows\n",
    "\n",
    "unfunded_trade_v1_filter['Waterfall_Number'] = unfunded_trade_v1_filter['days_diff'].apply(classify_loan_timing)\n",
    "\n",
    "min_waterfall = unfunded_trade_v1_filter.groupby('ZEST_KEY')['Waterfall_Number'].transform('min')\n",
    "\n",
    "# Keep only rows where the waterfall_number is the minimum for that ZEST_KEY\n",
    "nonmortagesecure_loan = unfunded_trade_v1_filter[unfunded_trade_v1_filter['Waterfall_Number'] == min_waterfall].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7782099, 63), (10367, 65), 8880, (9347, 65))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfunded_trade_v1.shape, unfunded_trade_v1_filter.shape, unfunded_trade_v1_filter.ZEST_KEY.nunique(), nonmortagesecure_loan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waterfall_Number\n",
       "1    4087\n",
       "2    2802\n",
       "3    2458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonmortagesecure_loan['Waterfall_Number'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.Existing Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((352536, 179), (343656, 179))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfunded_ana_v1.shape, unfunded_ana_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE_OPENED is already of datetime format! Will skip the converting process\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7507616, 64), (4034304, 64))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfunded_ana_v2 = unfunded_ana_v1[~unfunded_ana_v1.index.isin(nonmortagesecure_loan.ZEST_KEY)]\n",
    "unfunded_trade_v2 = unfunded_trade[unfunded_trade.ZEST_KEY.isin(unfunded_ana_v2.index)]\n",
    "unfunded_trade_v2 = unfunded_trade_v2.merge(unfunded_ana_v2.reset_index(drop=False)[cols], how='left', on=['ZEST_KEY'])\n",
    "\n",
    "dates = {\n",
    "    'DATE_REPORTED': '%m%d%Y',\n",
    "    'DATE_OPENED': '%m%d%Y',\n",
    "}\n",
    "unfunded_trade_v2 = udpate_date_formats(unfunded_trade_v2, dates)\n",
    "unfunded_trade_v2['payment_pattern'] = unfunded_trade_v2['PAYMENT_HISTORY_1_24'] + unfunded_trade_v2['PAYMENT_HISTORY_25_36'] + unfunded_trade_v2['PAYMENT_HISTORY_37_48']\n",
    "unfunded_trade_v2['payment_pattern'] = unfunded_trade_v2['payment_pattern'].fillna('')\n",
    "unfunded_trade_v2['payment_pattern'] = unfunded_trade_v2['payment_pattern'].str.replace('/', '')\n",
    "\n",
    "# keep open loans\n",
    "unfunded_trade_v2_filter = unfunded_trade_v2[unfunded_trade_v2['CLOSED_DATE'].notna()]\n",
    "unfunded_trade_v2.shape, unfunded_trade_v2_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute periods\n",
    "unfunded_trade_v2_filter['app_month'] = pd.to_datetime(unfunded_trade_v2_filter['appDate']).dt.to_period('M')\n",
    "unfunded_trade_v2_filter['report_month'] = pd.to_datetime(unfunded_trade_v2_filter['DATE_REPORTED']).dt.to_period('M')\n",
    "\n",
    "# Compute month offset (how many months between report and app date)\n",
    "month_offset = (unfunded_trade_v2_filter['report_month'] - unfunded_trade_v2_filter['app_month']).apply(lambda x: x.n)\n",
    "\n",
    "# If payment_pattern is a string, we can extract using string indexing\n",
    "# Invalid indexes will be caught later\n",
    "pattern_array = unfunded_trade_v2_filter['payment_pattern'].astype(str).values\n",
    "offset_array = month_offset.values\n",
    "\n",
    "# Define fast numpy function to extract the code\n",
    "def get_char(patterns, offsets):\n",
    "    result = np.full(len(patterns), None, dtype=object)\n",
    "    for i in range(len(patterns)):\n",
    "        offset = offsets[i]\n",
    "        if 0 <= offset < len(patterns[i]):\n",
    "            result[i] = patterns[i][offset]\n",
    "    return result\n",
    "\n",
    "# Apply it\n",
    "unfunded_trade_v2_filter['status_at_appmonth'] = get_char(pattern_array, offset_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoloan_account_types = {\n",
    "        \"3A\": \"Auto lease\",\n",
    "        \"00\": \"Auto loan\",\n",
    "        \"11\": \"Recreational merchandise loan\"\n",
    "    }\n",
    "personalloan_account_types = {\n",
    "        \"01\": \"Unsecured loan\",\n",
    "        \"02\": \"Secured loan\",\n",
    "        \"03\": \"Partially secured loan\",\n",
    "        \"06\": \"Installment sales contract\",\n",
    "        \"0A\": \"Time-share loan\",\n",
    "        \"0F\": \"Construction loan\",\n",
    "        \"13\": \"Lease\",\n",
    "        \"15\": \"Check, credit or line of credit\",\n",
    "        \"20\": \"Note loan\",\n",
    "        \"29\": \"Rental agreement\",\n",
    "        \"47\": \"Credit line secured revolving terms\",\n",
    "        \"4D\": \"Telecommunications/Cellular\",\n",
    "        \"70\": \"Government overpayment\",\n",
    "        \"7B\": \"Agriculture\",\n",
    "        \"92\": \"Utility company\",\n",
    "        \"78\": \"Installment loan\",\n",
    "        \"91\": \"Debt consolidation \"\n",
    "    }\n",
    "\n",
    "unfunded_trade_v2_paidasagreed = unfunded_trade_v2_filter[unfunded_trade_v2_filter['status_at_appmonth'].isin([\"0\",\"1\",\"E\"])]\n",
    "unfunded_trade_v2_paidasagreed['Waterfall_Number'] = np.where(\n",
    "    unfunded_trade_v2_paidasagreed['ACCOUNT_TYPE'].isin(autoloan_account_types),\n",
    "    4,\n",
    "    np.where(\n",
    "        unfunded_trade_v2_paidasagreed['ACCOUNT_TYPE'].isin(personalloan_account_types),\n",
    "        5,\n",
    "        6\n",
    "    )\n",
    ")\n",
    "\n",
    "min_waterfall = unfunded_trade_v2_paidasagreed.groupby('ZEST_KEY')['Waterfall_Number'].transform('min')\n",
    "# Keep only rows where the waterfall_number is the minimum for that ZEST_KEY\n",
    "existing_trade = unfunded_trade_v2_paidasagreed[unfunded_trade_v2_paidasagreed['Waterfall_Number'] == min_waterfall].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waterfall_Number\n",
       "4    191569\n",
       "6    187168\n",
       "5     27685\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_trade['Waterfall_Number'].value_counts(dropna = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9347, 65) (406422, 68)\n",
      "(415769, 64)\n"
     ]
    }
   ],
   "source": [
    "print(nonmortagesecure_loan.shape, existing_trade.shape)\n",
    "common_cols = list(set(nonmortagesecure_loan.columns) & set(existing_trade.columns))\n",
    "proxy_trade_waterfall = pd.concat([nonmortagesecure_loan, existing_trade])[common_cols].reset_index(drop=True)\n",
    "print(proxy_trade_waterfall.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_proxy_time_window = [-36500, 36500]\n",
    "proxy_account_type = list(set(proxy_trade_waterfall['ACCOUNT_TYPE']))\n",
    "target_generator = EQTargetGenerator(join_key='ZEST_KEY', app_id='appId', app_date='appDate', \n",
    "                                      proxy_time_window = default_proxy_time_window, \n",
    "                                      proxy_account_type = proxy_account_type,\n",
    "                                      **eq_trade_json['info'],\n",
    "                                     proxy = True)\n",
    "target_generator_nobnk = EQTargetGenerator(join_key='ZEST_KEY', app_id='appId', app_date='appDate', \n",
    "                                      proxy_time_window = default_proxy_time_window, \n",
    "                                      proxy_account_type = proxy_account_type,\n",
    "                                      **eq_trade_json_nobnk['info'],\n",
    "                                     proxy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Transforming date formats ---\n",
      "Warning! Cannot covert CLOSED_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_BEFORE_HISTORY to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert LAST_PAYMENT_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DFD_DLA to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_3 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DMD_REPORTED to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "DATE_OPENED is already of datetime format! Will skip the converting process\n",
      "Warning! Cannot covert DEFERRED_PAYMENT_START_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "DATE_REPORTED is already of datetime format! Will skip the converting process\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_2 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_1 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "--- Selecting loan types in ['8B', '01', '8A', '10', '22', None, '93', '3A', '9A', '20', '6B', '00', '2A', '47', '17', '0A', '5B', '13', '92', '03', '50', '1C', '6D', '90', '23', '5A', '66', '11', '04', '95', '0G', '89', '70', '07', '06', '02', '7B', '19', '2C', '29', '7A', '08', '26', '18', '12', '43', '6A', '25', '15', '91', '0F'] ---\n",
      "--- Selecting loan periods within [-36500, 36500] days ---\n",
      "--- Implementing additional data preprocess ---\n",
      "--- Getting DQ/CO dates from: ---\n",
      "    get_coAmt_dq_date\n",
      "    get_code_dq_date\n",
      "    get_dmd_dq_date\n",
      "    get_highRate_dq_date\n",
      "    get_pattern_dq_date\n",
      "--- Implementing additional proxy date postprocess ---\n",
      "--- Transforming date formats ---\n",
      "Warning! Cannot covert CLOSED_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_BEFORE_HISTORY to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert LAST_PAYMENT_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DFD_DLA to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_3 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert DMD_REPORTED to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "DATE_OPENED is already of datetime format! Will skip the converting process\n",
      "Warning! Cannot covert DEFERRED_PAYMENT_START_DATE to datetime using %m%d%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "DATE_REPORTED is already of datetime format! Will skip the converting process\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_2 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "Warning! Cannot covert PREVIOUS_HIGH_DATE_1 to datetime using %m%Y format! Please check the asset dates format!\n",
      "Trying to convert datetime using inferred datetime format...\n",
      "--- Selecting loan types in ['8B', '01', '8A', '10', '22', None, '93', '3A', '9A', '20', '6B', '00', '2A', '47', '17', '0A', '5B', '13', '92', '03', '50', '1C', '6D', '90', '23', '5A', '66', '11', '04', '95', '0G', '89', '70', '07', '06', '02', '7B', '19', '2C', '29', '7A', '08', '26', '18', '12', '43', '6A', '25', '15', '91', '0F'] ---\n",
      "--- Selecting loan periods within [-36500, 36500] days ---\n",
      "--- Implementing additional data preprocess ---\n",
      "--- Getting DQ/CO dates from: ---\n",
      "    get_coAmt_dq_date\n",
      "    get_code_dq_date\n",
      "    get_dmd_dq_date\n",
      "    get_highRate_dq_date\n",
      "    get_pattern_dq_date\n",
      "--- Implementing additional proxy date postprocess ---\n"
     ]
    }
   ],
   "source": [
    "proxy_date_df_waterfall = target_generator.get_dqDates(proxy_trade_waterfall)\n",
    "proxy_date_df_waterfall_nobnk = target_generator_nobnk.get_dqDates(proxy_trade_waterfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415728, 103), (415728, 103))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_date_df_waterfall.shape, proxy_date_df_waterfall_nobnk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "proxy_target_waterfall = target_generator.generate_target(proxy_date_df_waterfall, \n",
    "                                         target=dqs, \n",
    "                                         time_window=time_window, \n",
    "                                         query_date= pull_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "proxy_target_waterfall_nobnk = target_generator_nobnk.generate_target(proxy_date_df_waterfall_nobnk, \n",
    "                                         target=dqs, \n",
    "                                         time_window=time_window, \n",
    "                                         query_date= pull_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415728, 405), (415728, 405))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall.shape, proxy_target_waterfall_nobnk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnk_column_mapping = \\\n",
    "    {'proxy_DQ60_m6': 'mob_6_60',\n",
    "     'proxy_DQ60_m12': 'mob_12_60',\n",
    "      'proxy_DQ90_m12': 'mob_12_90',\n",
    "      'proxy_DQ90_m18': 'mob_18_90',\n",
    "     'proxy_DQ120_m12': 'mob_12_120',\n",
    "    'proxy_DQ120_m18': 'mob_18_120',\n",
    "    'proxy_DQ120_m24': 'mob_24_120',\n",
    "      'proxy_DQ60_m6_status': 'mob_6_60_status',\n",
    "      'proxy_DQ60_m12_status': 'mob_12_60_status',\n",
    "      'proxy_DQ90_m12_status': 'mob_12_90_status',\n",
    "      'proxy_DQ90_m18_status': 'mob_18_90_status',\n",
    "     'proxy_DQ120_m12_status': 'mob_12_120_status',\n",
    "    'proxy_DQ120_m18_status': 'mob_18_120_status',\n",
    "    'proxy_DQ120_m24_status': 'mob_24_120_status',\n",
    "}\n",
    "\n",
    "nobnk_column_mapping = {\n",
    "    'proxy_DQ120_m12': 'mob_12_120_nobnk',\n",
    "    'proxy_DQ120_m18': 'mob_18_120_nobnk',\n",
    "    'proxy_DQ120_m24': 'mob_24_120_nobnk',\n",
    "    'proxy_DQ120_m12_status': 'mob_12_120_nobnk_status',\n",
    "    'proxy_DQ120_m18_status': 'mob_18_120_nobnk_status',\n",
    "    'proxy_DQ120_m24_status': 'mob_24_120_nobnk_status',\n",
    "}\n",
    "\n",
    "date_source_cols = [c for c in proxy_target_waterfall.columns if c.endswith('date_source')]\n",
    "\n",
    "proxy_target_waterfall_rename = proxy_target_waterfall.rename(columns=bnk_column_mapping)\n",
    "proxy_target_waterfall_nobnk_rename = proxy_target_waterfall_nobnk.rename(columns=nobnk_column_mapping)\n",
    "\n",
    "proxy_target_waterfall_nobnk_keep_cols = list(nobnk_column_mapping.values())\n",
    "\n",
    "proxy_target_waterfall_all = pd.concat([proxy_target_waterfall_rename, \\\n",
    "                                       proxy_target_waterfall_nobnk_rename[proxy_target_waterfall_nobnk_keep_cols]\n",
    "                                       ], axis=1)\n",
    "proxy_target_waterfall_all['abs_days_diff'] = abs((proxy_target_waterfall_all['DATE_OPENED'] - proxy_target_waterfall_all['appDate']).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415728, 355), (415728, 405), (415728, 405))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall_all.shape, proxy_target_waterfall.shape, proxy_target_waterfall_nobnk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all = pd.read_parquet('/d/shared/users/yyz/penfed/proxy_target_waterfall_all.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose worst performing trade (60+DPD M12), next choose trade opened closest to app month\n",
    "# Else random select from duplicate trades\n",
    "proxy_target_waterfall_all_dedup = proxy_target_waterfall_all.sort_values(by=['ZEST_KEY','mob_12_60', 'abs_days_diff'], ascending=[True, False, True])\\\n",
    "    .drop_duplicates(subset=['ZEST_KEY'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all.to_parquet('/d/shared/users/yyz/penfed/proxy_target_waterfall_all.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((275340, 356), 275340)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall_all_dedup.shape, proxy_target_waterfall_all_dedup['ZEST_KEY'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (21,42,60,61,71,72,122) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "keep_cols = ['ZEST_KEY','Waterfall_Number','DATE_OPENED','DATE_REPORTED','ACCOUNT_TYPE','HIGH_CREDIT','SCHEDULED_PAYMENT_AMOUNT','TERMS_DURATION','TERMS_FREQUENCY',\\\n",
    "             'PAYMENT_HISTORY_1_24','PAYMENT_HISTORY_25_36','PAYMENT_HISTORY_37_48'] + \\\n",
    "            list(bnk_column_mapping.values())+list(nobnk_column_mapping.values())\n",
    "proxy_target_waterfall_all_dedup_subset = proxy_target_waterfall_all_dedup[keep_cols]\n",
    "\n",
    "df_raw = pd.read_csv('/d/shared/silver_projects_v2/penfed/autoindirectv1/shared_data/equifax/raw/client/data_3rd_party_refi_f_sent_20250523183131_withhasedappId.csv')\n",
    "\n",
    "df_raw['flgBorrower'] = df_raw['Role_Code'].map({'PRIMARY': 1, 'JOINTOWN': 2})\n",
    "df_raw['ZEST_KEY'] = df_raw['Application_Number'].astype(str)+'_'+df_raw['flgBorrower'].astype(str)+'_229_22'\n",
    "keep_cols = ['ZEST_KEY','Application_Number_hashed','Application_Date','Role_Code','Seller_Name','Product','Application_Status','PenFed_Funded']\n",
    "df_raw = df_raw[keep_cols]\n",
    "proxy_target_waterfall_all_dedup_merge = df_raw.merge(proxy_target_waterfall_all_dedup_subset, how='inner', on=['ZEST_KEY'])\\\n",
    "    .drop(columns=['ZEST_KEY','mob_12_120','mob_18_120','mob_24_120','mob_12_120_status','mob_18_120_status','mob_24_120_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275340, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall_all_dedup_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all_dedup_merge.to_csv('/d/shared/users/yyz/penfed/proxy_target_waterfall_part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all_dedup_merge = pd.read_csv('/d/shared/users/yyz/penfed/proxy_target_waterfall_part2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = proxy_target_waterfall_all_dedup_merge.copy()\n",
    "\n",
    "import numpy_financial as npf\n",
    "trade['TERMS_DURATION'] = trade['TERMS_DURATION'].astype(float)\n",
    "trade['SCHEDULED_PAYMENT_AMOUNT'] = trade['SCHEDULED_PAYMENT_AMOUNT'].astype(float)\n",
    "trade['HIGH_CREDIT'] = trade['HIGH_CREDIT'].astype(float)\n",
    "\n",
    "\n",
    "# generate a numeric value for the term frequency; assume monthy if missing\n",
    "trade['TERMS_FREQUENCY'] = trade['TERMS_FREQUENCY'].fillna('M')\n",
    "freq_map = {'B':26,'E':24,'L':6,'M':12,'P':1,'Q':4,'S':2,'T':3,'W':52,'Y':1}\n",
    "trade['TERMS_FREQUENCY_numeric'] = trade['TERMS_FREQUENCY'].map(freq_map)\n",
    "\n",
    "# filter out invalid rows\n",
    "## drop rows where bureau loan terms are missing or 0\n",
    "trade = trade.dropna(subset=['TERMS_DURATION', 'TERMS_FREQUENCY', 'HIGH_CREDIT', 'SCHEDULED_PAYMENT_AMOUNT'])\n",
    "trade = trade[(trade[['TERMS_DURATION', 'SCHEDULED_PAYMENT_AMOUNT', 'HIGH_CREDIT']] > 0).any(axis=1)]\n",
    "\n",
    "## drop rows if the full scheduled payment amount is higher than 3 times the loan amount (entries could be a typo and calculated APRs would be too high)\n",
    "mask = (trade['TERMS_DURATION'] * trade['SCHEDULED_PAYMENT_AMOUNT'] / trade['HIGH_CREDIT']) < 3\n",
    "trade = trade[mask]\n",
    "## drop rows if the full scheduled payment amount is less than or equal to the loan amount (entries could be a typo and calculated APRs would be too low)\n",
    "mask2 = (trade['TERMS_DURATION'] * trade['SCHEDULED_PAYMENT_AMOUNT'] / trade['HIGH_CREDIT']) >= 1\n",
    "trade = trade[mask2]\n",
    "\n",
    "# calculate the APR\n",
    "trade['Estimated_APR'] = npf.rate(\n",
    "    trade['TERMS_DURATION'],\n",
    "    -trade['SCHEDULED_PAYMENT_AMOUNT'],\n",
    "    trade['HIGH_CREDIT'],\n",
    "    0,\n",
    "    maxiter=100\n",
    "    ) * trade['TERMS_FREQUENCY_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all_dedup_merge_apr = proxy_target_waterfall_all_dedup_merge.merge(trade[['Application_Number_hashed', 'Role_Code', 'Estimated_APR']], \\\n",
    "                                             how='left', on=['Application_Number_hashed', 'Role_Code'])\n",
    "proxy_target_waterfall_all_dedup_merge_apr = proxy_target_waterfall_all_dedup_merge_apr.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_target_waterfall_all_dedup_merge_apr.to_csv('/d/shared/users/yyz/penfed/proxy_target_waterfall_part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((275340, 32), (275339, 33))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall_all_dedup_merge.shape, proxy_target_waterfall_all_dedup_merge_apr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "275526 - 275340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Application_Number_hashed                                         Role_Code\n",
       "f0d99b37773cd5b43ccd7e6327446a5dfc807becf04cf48350da2c8d03463144  PRIMARY      2\n",
       "Name: Application_Date, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = proxy_target_waterfall_all_dedup_merge.groupby(['Application_Number_hashed', 'Role_Code'])['Application_Date'].count()\n",
    "a[a>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = proxy_target_waterfall_all_dedup_merge[proxy_target_waterfall_all_dedup_merge['Application_Number_hashed']=='f0d99b37773cd5b43ccd7e6327446a5dfc807becf04cf48350da2c8d03463144']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "appmonth\n",
       "2021-01      7.0\n",
       "2021-02      7.0\n",
       "2021-03     23.0\n",
       "2021-04     26.0\n",
       "2021-05     31.0\n",
       "2021-06     43.0\n",
       "2021-07     75.0\n",
       "2021-08     61.0\n",
       "2021-09     70.0\n",
       "2021-10    147.0\n",
       "2021-11    141.0\n",
       "2021-12    131.0\n",
       "2022-01    169.0\n",
       "2022-02    153.0\n",
       "2022-03    228.0\n",
       "2022-04    246.0\n",
       "2022-05    284.0\n",
       "2022-06    294.0\n",
       "2022-07    139.0\n",
       "2022-08     58.0\n",
       "2022-09     61.0\n",
       "2022-10     43.0\n",
       "2022-11     54.0\n",
       "2022-12     53.0\n",
       "2023-01     45.0\n",
       "2023-02     43.0\n",
       "2023-03     53.0\n",
       "2023-04     58.0\n",
       "2023-05     63.0\n",
       "2023-06     27.0\n",
       "2023-07     21.0\n",
       "2023-08     21.0\n",
       "2023-09     16.0\n",
       "2023-10     11.0\n",
       "2023-11     11.0\n",
       "2023-12      6.0\n",
       "2024-01     15.0\n",
       "2024-02      7.0\n",
       "2024-03      9.0\n",
       "2024-04      8.0\n",
       "2024-05      3.0\n",
       "2024-06      0.0\n",
       "2024-07      1.0\n",
       "2024-08      0.0\n",
       "2024-09      2.0\n",
       "2024-10      2.0\n",
       "2024-11      1.0\n",
       "2024-12      0.0\n",
       "2025-01      0.0\n",
       "2025-02      1.0\n",
       "2025-03      1.0\n",
       "Name: mob_24_120_nobnk, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxy_target_waterfall_all_dedup_merge['appmonth'] = proxy_target_waterfall_all_dedup_merge['Application_Date'].str[:7]\n",
    "proxy_target_waterfall_all_dedup_merge.groupby('appmonth')['mob_24_120_nobnk'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310dp",
   "language": "python",
   "name": "py310dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
