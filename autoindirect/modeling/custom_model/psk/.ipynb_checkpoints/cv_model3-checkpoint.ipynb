{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Data Only Model clientmodel3a\n",
    "\n",
    "- feature selection on PenFed data +org data\n",
    "- with LTV application feature\n",
    "- train test split: \n",
    "  \"train\": {\"start_date\": \"2021-07-01\", \"end_date\": \"2022-07-01\"},\n",
    "  \"valid\": {\"start_date\": \"2022-07-01\", \"end_date\": \"2023-01-01\"},\n",
    "  \"test\": {\"start_date\": \"2023-01-01\", \"end_date\": \"2024-01-01\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House keeping settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('v1.13.1', '34.4.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from model_engine.assets.utils import load_asset\n",
    "from model_engine.io.loaders import load_json\n",
    "from model_engine.power.post_sale import PowerModelBuilder\n",
    "from model_engine.analysis.suggesters import evaluate_datesplits\n",
    "import warnings\n",
    "\n",
    "from zaml.common.utils import load_state\n",
    "\n",
    "import model_engine, zaml\n",
    "model_engine.__version__, zaml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_engine.model_builder.asset_parser import asset_parser\n",
    "from model_engine.model_builder import ModelBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zaml.model.modeling import XGBoostModel\n",
    "from zaml.model.model_selection.parameter_search import ParameterSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = 'clientmodel3a'\n",
    "input_target_name = 'final_DQ90_m12'\n",
    "\n",
    "base_model_output_path = f'/d/shared/silver_projects_v2/penfed/autoindirectv1/modeling/{base_model_id}'\n",
    "\n",
    "\n",
    "train_app = pd.read_parquet(os.path.join(base_model_output_path, 'train_app.parquet')).sort_values('appDate')\n",
    "valid_app = pd.read_parquet(os.path.join(base_model_output_path, 'valid_app.parquet')).sort_values('appDate')\n",
    "\n",
    "\n",
    "train_fe_data = pd.read_parquet(os.path.join(base_model_output_path, 'train_fe_data.parquet')).merge(train_app[['ZEST_KEY','appDate']])\n",
    "train_target = pd.read_parquet(os.path.join(base_model_output_path, 'train_target.parquet'))\n",
    "has_target_list = list(train_target[train_target['target'].notna()]['ZEST_KEY'])\n",
    "train = train_fe_data[train_fe_data['ZEST_KEY'].isin(has_target_list)].merge(train_target).sort_values('appDate').set_index('ZEST_KEY')\n",
    "train_fe_data = train.drop(columns=['target'])\n",
    "train_target = train['target']\n",
    "\n",
    "\n",
    "valid_fe_data = pd.read_parquet(os.path.join(base_model_output_path, 'valid_fe_data.parquet')).merge(valid_app[['ZEST_KEY','appDate']])\n",
    "valid_target = pd.read_parquet(os.path.join(base_model_output_path, 'valid_target.parquet'))\n",
    "has_target_list = list(valid_target[valid_target['target'].notna()]['ZEST_KEY'])\n",
    "valid = valid_fe_data[valid_fe_data['ZEST_KEY'].isin(has_target_list)].merge(valid_target).sort_values('appDate').set_index('ZEST_KEY')\n",
    "valid_fe_data = valid.drop(columns=['target'])\n",
    "valid_target = valid['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-07-01 00:00:00'), Timestamp('2022-06-30 00:00:00'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_app['appDate'].min(), train_app['appDate'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appDate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07</td>\n",
       "      <td>8882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08</td>\n",
       "      <td>8347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09</td>\n",
       "      <td>8304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10</td>\n",
       "      <td>10292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11</td>\n",
       "      <td>9027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-12</td>\n",
       "      <td>8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>17786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02</td>\n",
       "      <td>18652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03</td>\n",
       "      <td>23850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>49804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-08</td>\n",
       "      <td>53381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-09</td>\n",
       "      <td>53099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-10</td>\n",
       "      <td>54492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-11</td>\n",
       "      <td>57971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-12</td>\n",
       "      <td>57886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>65346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-02</td>\n",
       "      <td>68632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-03</td>\n",
       "      <td>86826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-04</td>\n",
       "      <td>83881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-05</td>\n",
       "      <td>86409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-06</td>\n",
       "      <td>87966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    appDate  count\n",
       "0   2020-07   8882\n",
       "1   2020-08   8347\n",
       "2   2020-09   8304\n",
       "3   2020-10  10292\n",
       "4   2020-11   9027\n",
       "5   2020-12   8594\n",
       "6   2021-01  17786\n",
       "7   2021-02  18652\n",
       "8   2021-03  23850\n",
       "9   2021-07  49804\n",
       "10  2021-08  53381\n",
       "11  2021-09  53099\n",
       "12  2021-10  54492\n",
       "13  2021-11  57971\n",
       "14  2021-12  57886\n",
       "15  2022-01  65346\n",
       "16  2022-02  68632\n",
       "17  2022-03  86826\n",
       "18  2022-04  83881\n",
       "19  2022-05  86409\n",
       "20  2022-06  87966"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_app \n",
    "df['appDate'] = pd.to_datetime(df['appDate'])\n",
    "\n",
    "monthly_counts = (\n",
    "    df\n",
    "    .groupby(df['appDate'].dt.to_period('M'))\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "monthly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229856, 19)\n",
      "2020-07-01 00:00:00 2021-09-08 00:00:00\n",
      "(459712, 19)\n",
      "2020-07-01 00:00:00 2022-01-11 00:00:00\n",
      "(689568, 19)\n",
      "2020-07-01 00:00:00 2022-04-11 00:00:00\n",
      "(919427, 19)\n",
      "2020-07-01 00:00:00 2022-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Determine the folds and check the counts in each fold \n",
    "\n",
    "folds = []\n",
    "fold_size = len(df) // 4\n",
    "\n",
    "for i in range(4):\n",
    "    start_idx = i * fold_size\n",
    "    if i < 3:\n",
    "        end_idx = (i + 1) * fold_size\n",
    "    else:\n",
    "        end_idx = len(df)  \n",
    "    print(df.iloc[0:end_idx].shape)\n",
    "    print(df.iloc[0:end_idx].appDate.min(), df.iloc[0:end_idx].appDate.max())\n",
    "    folds.append(df.iloc[0:end_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2022-07-01 00:00:00'), Timestamp('2022-12-31 00:00:00'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_app['appDate'].min(), valid_app['appDate'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([train_fe_data, valid_fe_data], axis=0)\n",
    "y_combined = pd.concat([train_target, valid_target], axis=0)\n",
    "\n",
    "X_combined = X_combined.reset_index(drop=True)\n",
    "y_combined = y_combined.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Define date ranges for splits\n",
    "date_splits = [\n",
    "    (\"2020-07-01\", \"2021-11-01\", \"2021-11-01\", \"2022-02-01\"),\n",
    "    (\"2020-07-01\", \"2022-02-01\", \"2022-02-01\", \"2022-04-15\"),\n",
    "    (\"2020-07-01\", \"2022-04-15\", \"2021-04-15\", \"2022-07-01\"),\n",
    "    (\"2020-07-01\", \"2022-07-01\", \"2022-07-01\", \"2022-12-31\")\n",
    "]\n",
    "\n",
    "custom_splits = []\n",
    "\n",
    "for train_start, train_end, val_start, val_end in date_splits:\n",
    "    # Create boolean masks for train and validation splits\n",
    "    train_mask = (X_combined['appDate'] >= train_start) & (X_combined['appDate'] < train_end)\n",
    "    val_mask = (X_combined['appDate'] >= val_start) & (X_combined['appDate'] < val_end)\n",
    "    \n",
    "    # Get indices\n",
    "    train_indices = np.where(train_mask)[0]\n",
    "    val_indices = np.where(val_mask)[0]\n",
    "    \n",
    "    custom_splits.append((train_indices, val_indices))\n",
    "    \n",
    "\n",
    "X_combined = X_combined.drop(columns=['appDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_space = {\n",
    "    \"max_depth\": [3,4,5,6],\n",
    "    \"subsample\": [0.1, 0.3, 0.5, 0.7],\n",
    "    \"scale_pos_weight\": [2.5, 3, 4],\n",
    "    \"colsample_bytree\": [0.05, 0.1, 0.2, 0.25],\n",
    "    \"min_child_weight\": [50, 100, 150, 200, 250, 300, 350],\n",
    "}\n",
    "fit_params = {\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.01,\n",
    "    'early_stopping_rounds': 200,      # Early stopping parameter\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m default_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_pos_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2.5\u001b[39m,\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m350\u001b[39m,\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.05\u001b[39m}\n\u001b[1;32m     20\u001b[0m found_flg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom_params\u001b[49m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m==\u001b[39mdefault_params:\n\u001b[1;32m     23\u001b[0m         found_flg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_params' is not defined"
     ]
    }
   ],
   "source": [
    "columns = ['subsample','scale_pos_weight','min_child_weight','max_depth','colsample_bytree'] + ['avg_train_score','std_train_score','avg_val_score','std_val_score']\n",
    "\n",
    "# File path for the CSV\n",
    "output_file = f\"./tuning_submodel_weights_psk/{base_model_id}_cv_results.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "file_exists = os.path.isfile(output_file)\n",
    "\n",
    "# Open the file for appending, writing the header only if it doesn't exist\n",
    "if not file_exists:\n",
    "    with open(output_file, mode='w') as f:\n",
    "        pd.DataFrame(columns=columns).to_csv(f, index=False)\n",
    "\n",
    "default_params = {'subsample': 0.5,\n",
    "  'scale_pos_weight': 2.5,\n",
    "  'min_child_weight': 350,\n",
    "  'max_depth': 3,\n",
    "  'colsample_bytree': 0.05}\n",
    "\n",
    "# found_flg = False\n",
    "# for params in random_params:\n",
    "#     if params==default_params:\n",
    "#         found_flg = True\n",
    "#         break\n",
    "# print(found_flg)\n",
    "# if ~found_flg:\n",
    "#     random_params.append(default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 4, 'min_child_weight': 150, 'max_depth': 6, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8906258323347278, Validation AUC: 0.8066667617405093\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 4, 'min_child_weight': 150, 'max_depth': 6, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 4, 'min_child_weight': 50, 'max_depth': 6, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8948327079374137, Validation AUC: 0.8089910708049153\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 4, 'min_child_weight': 50, 'max_depth': 6, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 2.5, 'min_child_weight': 200, 'max_depth': 6, 'colsample_bytree': 0.2}\n",
      "Train AUC: 0.8811527437142684, Validation AUC: 0.8069365569643674\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 2.5, 'min_child_weight': 200, 'max_depth': 6, 'colsample_bytree': 0.2}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 3, 'min_child_weight': 200, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8667046169541335, Validation AUC: 0.8002610087154387\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 3, 'min_child_weight': 200, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 250, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8499042057182945, Validation AUC: 0.7961196309892529\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 250, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.2}\n",
      "Train AUC: 0.8491306408837699, Validation AUC: 0.7977622786203108\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.2}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 100, 'max_depth': 6, 'colsample_bytree': 0.2}\n",
      "Train AUC: 0.8884588182145078, Validation AUC: 0.8099673891293235\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 100, 'max_depth': 6, 'colsample_bytree': 0.2}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 4, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8507728680722768, Validation AUC: 0.7949479009876314\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 4, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 2.5, 'min_child_weight': 250, 'max_depth': 5, 'colsample_bytree': 0.25}\n",
      "Train AUC: 0.8495792325884591, Validation AUC: 0.7984666066213248\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 2.5, 'min_child_weight': 250, 'max_depth': 5, 'colsample_bytree': 0.25}\n",
      "Testing parameters: {'subsample': 0.1, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8043192141120691, Validation AUC: 0.7775062048310137\n",
      "Saved results for parameters: {'subsample': 0.1, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.1, 'scale_pos_weight': 2.5, 'min_child_weight': 50, 'max_depth': 6, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8559565886248885, Validation AUC: 0.8014479269400915\n",
      "Saved results for parameters: {'subsample': 0.1, 'scale_pos_weight': 2.5, 'min_child_weight': 50, 'max_depth': 6, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 50, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8923106727280964, Validation AUC: 0.8062547797822045\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 50, 'max_depth': 5, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.1, 'scale_pos_weight': 4, 'min_child_weight': 300, 'max_depth': 5, 'colsample_bytree': 0.2}\n",
      "Train AUC: 0.8247881911427211, Validation AUC: 0.7884817780444504\n",
      "Saved results for parameters: {'subsample': 0.1, 'scale_pos_weight': 4, 'min_child_weight': 300, 'max_depth': 5, 'colsample_bytree': 0.2}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 4, 'min_child_weight': 100, 'max_depth': 5, 'colsample_bytree': 0.25}\n",
      "Train AUC: 0.8868667487791152, Validation AUC: 0.8085098999514173\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 4, 'min_child_weight': 100, 'max_depth': 5, 'colsample_bytree': 0.25}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 4, 'min_child_weight': 50, 'max_depth': 4, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8717215164405319, Validation AUC: 0.8023876837541182\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 4, 'min_child_weight': 50, 'max_depth': 4, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 200, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8420756363492891, Validation AUC: 0.7914008031188041\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 200, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 100, 'max_depth': 3, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8408339554175871, Validation AUC: 0.7925964988497135\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 3, 'min_child_weight': 100, 'max_depth': 3, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.8636057377217179, Validation AUC: 0.7979213717012962\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 2.5, 'min_child_weight': 350, 'max_depth': 5, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8601938521999223, Validation AUC: 0.7993657730986152\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 2.5, 'min_child_weight': 350, 'max_depth': 5, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 200, 'max_depth': 3, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8375003059178249, Validation AUC: 0.7918828838047898\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 200, 'max_depth': 3, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.835304654506234, Validation AUC: 0.7897967581833629\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 3, 'min_child_weight': 350, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.3, 'scale_pos_weight': 2.5, 'min_child_weight': 250, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Train AUC: 0.830815426925229, Validation AUC: 0.7884408372605658\n",
      "Saved results for parameters: {'subsample': 0.3, 'scale_pos_weight': 2.5, 'min_child_weight': 250, 'max_depth': 3, 'colsample_bytree': 0.05}\n",
      "Testing parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 300, 'max_depth': 5, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8525732713793165, Validation AUC: 0.7986614570921162\n",
      "Saved results for parameters: {'subsample': 0.5, 'scale_pos_weight': 2.5, 'min_child_weight': 300, 'max_depth': 5, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.1}\n",
      "Train AUC: 0.8614705484111252, Validation AUC: 0.7994133113447704\n",
      "Saved results for parameters: {'subsample': 0.7, 'scale_pos_weight': 3, 'min_child_weight': 150, 'max_depth': 4, 'colsample_bytree': 0.1}\n",
      "Testing parameters: {'subsample': 0.7, 'scale_pos_weight': 4, 'min_child_weight': 100, 'max_depth': 5, 'colsample_bytree': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "n_iter = 200  # Number of random samples\n",
    "random_params = list(ParameterSampler(params_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Loop over random parameter combinations\n",
    "for params in random_params:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    # Temporal cross-validation\n",
    "    for train_idx, val_idx in custom_splits:\n",
    "        X_train, X_val = X_combined.iloc[train_idx], X_combined.iloc[val_idx]\n",
    "        y_train, y_val = y_combined.iloc[train_idx], y_combined.iloc[val_idx]\n",
    "        \n",
    "        combined_params = {**fit_params, **params}\n",
    "        \n",
    "        # Train the XGBoost model with early stopping\n",
    "        model = XGBoostModel(**combined_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            valid_data=(X_val, y_val)\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model on validation data\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_score = roc_auc_score(y_val, val_preds)\n",
    "        val_scores.append(val_score)\n",
    "        \n",
    "        # Evaluate the model on training data\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_score = roc_auc_score(y_train, train_preds)\n",
    "        train_scores.append(train_score)\n",
    "    \n",
    "    # Calculate average and standard deviation of scores\n",
    "    avg_train_score = np.mean(train_scores)\n",
    "    std_train_score = np.std(train_scores)\n",
    "    avg_val_score = np.mean(val_scores)\n",
    "    std_val_score = np.std(val_scores)\n",
    "    \n",
    "    print(f\"Train AUC: {avg_train_score}, Validation AUC: {avg_val_score}\")\n",
    "    \n",
    "    row_data = {\n",
    "        **params,\n",
    "        'avg_train_score': avg_train_score,\n",
    "        'std_train_score': std_train_score,\n",
    "        'avg_val_score': avg_val_score,\n",
    "        'std_val_score': std_val_score\n",
    "    }\n",
    "    row_df = pd.DataFrame([row_data])\n",
    "\n",
    "    # Append the row to the CSV file\n",
    "    row_df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "    print(f\"Saved results for parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.read_csv(f'./tuning_submodel_weights_psk/{base_model_id}_cv_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default hyperparameter set from LargeClientPipelineFactory\n",
    "\n",
    "results.loc[(results['max_depth']==3)&(results['subsample']==0.5)&(results['scale_pos_weight']==2.5)&(results['colsample_bytree']==0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of hyperparameters that give smallest difference between train and valid score \n",
    "\n",
    "results['avg_val_score_truncate'] = round(results['avg_val_score'], 3) \n",
    "results.sort_values(['avg_val_score_truncate','avg_train_score'], ascending=[False,True]).iloc[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"max_depth\": ,\n",
    "    \"subsample\": ,\n",
    "    \"scale_pos_weight\": ,\n",
    "    \"colsample_bytree\": ,\n",
    "    \"min_child_weight\": ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfold Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update params in asset with the best_params from CV \n",
    "\n",
    "base_asset['config']['pipeline_factory']['model'] = {'zaml_class': 'XGBoostModel',\n",
    " 'params': {'n_estimators': 10000,\n",
    "  'learning_rate': 0.01,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.5,\n",
    "    \"scale_pos_weight\": 2.5,\n",
    "    \"colsample_bytree\": 0.2,\n",
    "    \"min_child_weight\": 100,\n",
    "  'seed': 12,\n",
    "  'early_stopping_rounds': 200,\n",
    "  'eval_metric': 'auc'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold Valid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_penfed",
   "language": "python",
   "name": "penfed_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
